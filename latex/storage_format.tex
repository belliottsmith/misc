\documentclass[fleqn]{article}

\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{xpatch}
\let\algIf\If
\usepackage{zed-csp}
\let\If\algIf
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\xpatchcmd{\algorithmic}{\itemsep\z@}{\itemsep=0.5ex plus1pt}{}{}
\algnewcommand{\LineComment}[1]{\Statex \hskip\ALG@thistlm #1}
\makeatother

\title{Cassandra Storage Format\\Proposal for 3.1\\Version 1}
\author{Benedict Elliott Smith}
\date{January 2015}

\begin{document}

\maketitle

\begin{abstract}
This document is a draft proposal for a generalised replacement for the sstable format used in Cassandra.
The goal is to deliver a framework building on top of a number of abstracted components, with which
any efficient sstable representation can be encapsulated. It is designed to directly align with modern CQL
data modelling, however it should also support thrift models at least as efficiently.
\end{abstract}

\section{Outline}
\small
This section will outline the components and some associated concepts, before they are all used to define a pseudocode implementation.
\\
\paragraph{Definitions}
\begin{itemize}
  \item (Column) $Group_N$\\[2pt]
    A subset of the columns defined on the table, that are stored together in the 
     corresponding $DataFile_N$. By default there will be one group containing all columns, 
     but rarely accessed columns or columnar layouts may be separated into their own group.
     The same mechanism can be used to separate the row index from row values by using
     an empty initial group. It also supports duplication of cell data, for different access
     patterns.
  \item $BufferLimit$\\[2pt]
    An sstable represents an ordered collection of partitions, however this doesn't
    actually require the data be in order on disk. For linear scan performance we want it
    to be \emph{almost} in order, and this parameter controls how flexible we are.
    This specifies the maximum amount of data we are permitted to require to buffer, 
    when performing such a linear scan, in order to yield partitions in order.
  \item $PartitionEpoch$, $RowEpoch$\\[2pt]
    Against each tier of our structure we will encode the minimum timestamp found in the next
    tier, so that each may be encoded more efficiently. The expectation is that this will permit 
    many rows to store either only a single byte, or even a single bit, to represent the timestamp.
  \item $ColId_N$\\[2pt]
    A translation from column to a unique integer id within each $Group_N$
    
\end{itemize}

\paragraph{Components}
\paragraph{}
These components will each have more than one implementation.
\begin{itemize}
  \item $PartitionIndex$\\[2pt]
    One per sstable.
    Performs a translation from $PartitionKey$ to a set of addresses in $DataFile_0$ that may contain
    data for the key. This translation is permitted to yield false positives, but should
    expect to produce either zero or one result on average, with no false negatives.
    Multiple keys may map to the same location in $DataFile_0$, which can be exploited by an implementation
    to produce a compact mapping. These keys will generally be adjacent, but some may be out-of-order
    due to page packing.
  \item $RowIndex$\\[2pt]
    One per partition. Accessed sequentially, located by a $PartitionIndex$ lookup, it performs 
    a translation from $RowKey$ to a $RowEpoch$ and a position to provide to each $ValueStore_N$
    to produce cell values.
  \item $ValueStore_N$\\[2pt]
    One per $Group_N$, per partition. A sequential chunk of bytes that combines the position retrieved
    from the $RowIndex$ with an offset stored in $DataFile_0$, the queried columns, the $ColId_N$ translation
    and the $RowEpoch$ to return cell values. We assume a property $Pos$ can be queried prior to insertion
    of a new row that returns the value we will store in $RowIndex$ for future lookups.
\end{itemize}

\section{Pseudocode Implementations}
\paragraph{Notation Key}
\subparagraph{}
\begin{tabular}{l l}
$X \Leftarrow Y$ & Write/Add/Append $Y$ To $X$\\
$X \dashleftarrow Y$ & Read $X$ From $Y$\\
$X \gets Y$ & Set $X$ to $Y$\\
$\langle A,B \rangle$ & A tuple containing the values $A$ and $B$\\
$\{Item | Constraint\}$ & The set of all $Item$ produced by $Constraint$\\
$S_N$ & $\equiv \{S_n | n \in N\}$\\
$Stmt_x | \forall x \in X$ & Execute statement $S$ for each $x$ in $X$\\
$\lvert X \rvert$ & $\equiv \mathbf{card}(X)$, i.e. the number of elements in $X$\\
\end{tabular}

\begin{algorithm}
\scriptsize
\caption{Writing}
\begin{algorithmic}[1]
\Procedure{Write}{$Partitions, Group_N$}
\State $PartitionIndex, Buffer \gets $ \Call{New}{}
\State $DataFile_{n} \gets $ \Call{New}{} $\mid \forall n \in N$
\Statex
\State $ColGroup \gets \{c \mapsto n \mid \forall c \in Group_{n}, \forall n \in N \}$
\State $ColId_{n} \gets \{c \mapsto i \mid \forall c, d \in G, d < c \implies d \mapsto j \wedge j < i \} \mid \forall n \in N, G \equiv Group_{n}$
\State $Metadata \Leftarrow ColGroup$
\State $Metadata \Leftarrow ColId$
\Statex
\For{$p \gets Partitions$} 
 \State $RowIndex \gets $ \Call{New}{}
 \State $ValueStore_{n} \gets $ \Call{New}{} $ \mid \forall n \in N$
 \State $PartitionEpoch \gets \min \{v.timestamp \mid v \in r.Values, r \in p.Rows\}$
 \Statex
 \For{$r \gets p.Rows$}
   \State $RowEpoch \gets \min \{v.timestamp \mid v \in r.Values\}$
   \State $EpochDelta \gets RowEpoch - PartitionEpoch$
   \State $RowIndex \Leftarrow \langle r.RowKey \mapsto \langle EpochDelta, \{ v.Pos \mid v \in ValueStore_{N} \}\rangle \rangle $ 
   \State $Values_{n} \gets \{ ColId(c) \mapsto r.Values(c) \mid c \in Group_{n} \} \mid \forall n \in N$
   \State $v.Time \gets v.Time - RowEpoch \mid \forall v \in \mathbf{ran}(Values_{n}), \forall n \in N$
   \State $ ValueStore_{n} \Leftarrow Values_{N} $
 \EndFor
\Statex
 \State $Pos_{n} \gets DataFile_{n}.Pos | \forall n \in N$
 \State $Buffer \Leftarrow \langle p.PartitionKey, PartitionEpoch, RowIndex, ValueStore_{N}, Pos_{N} \rangle$
 \State $Distance \gets \sum\limits_{n=0}^{N} (DataFile_{n}.pos - \min\limits_{\forall b \in Buffer} b.Pos_{n}) $
 \If {$Distance \geq BufferLimit$}
   \State $Flush \gets$ \Call{Select}{$Buffer$}
   \State \Call{Flush}{$Flush, PartitionIndex, DataFile_{N}$}
   \State $Buffer \gets Buffer \setminus Flush$
 \EndIf
\EndFor
\State \Call{Flush}{$Buffer, PartitionIndex, DataFile_{N}$}
\EndProcedure
\Statex
\Procedure{Flush}{$Buffer, PartitionIndex, DataFile_{N}$}
   \State $PartitionIndex \Leftarrow \langle $\Call{Token}{$PartitionKey$}$ \mapsto DataFile_{0}.Pos \rangle \mid \forall PartitionKey \in Buffer$
   \State $DataFile_{0} \Leftarrow \lvert Buffer \rvert$
   \State $DataFile_{0} \Leftarrow $ \Call{Token}{$PartitionKey$} $ \mid \forall \langle PartitionKey, \_,\_,\_\rangle \in Buffer \rangle$
   \State $Offset \gets 0$
   \For{$\langle PartitionKey, \_, \_, \_ \rangle \gets Buffer$}
     \State $DataFile_0 \Leftarrow Offset$
     \State $Offset \gets Offset + $ \Call{SizeOf}{$\langle PartitionKey, RowIndex, ValueStore_0, $ \textsc{Epoch}$,$ \{\textsc{Address}$^N$\}$\rangle$}
   \EndFor
   \For{$\langle PartitionKey, PartitionEpoch, RowIndex, ValueStore_{N} \rangle \gets Buffer$}
    \State $DataFile_{0} \Leftarrow \langle PartitionEpoch, PartitionKey \rangle$
    \State $Pos_{n} \gets DataFile_{n}.Pos \mid \forall n \in N$
    \State $Pos_{0} \gets Pos_0 + $ \Call{SizeOf}{$RowIndex$} $ + $ \Call{SizeOf}{\{\textsc{Address$^N$}\}}
    \State $DataFile_{0} \Leftarrow Pos_{N}$
    \State $DataFile_{0} \Leftarrow RowIndex$
    \State $DataFile_{n} \Leftarrow ValueStore_{n} \mid \forall n \in N$
    \State Pad $DataFile_{n}$ to a page boundary $\mid \forall n \in N$
   \EndFor
\EndProcedure
\Statex
\Function{Select}{$Buffer$}
\State $Selection \gets$ ?
\Ensure $Selection \subseteq Buffer$
\Ensure $s \in Selection \cdot s.Pos = \min\limits_{\forall b \in Buffer} b.Pos_{n}$
\Ensure ($\exists k \cdot \sum\limits_{s \in Selection} s.Size_{n} \approx k \times PageSize) \mid \forall n \in N $
\Ensure Future calls to \Call{Select}{} can also meet criteria
\State \Return $Selection$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\scriptsize
\caption{Reading}
\begin{algorithmic}[2]
\Function{Read}{$PartitionKey, Cols, RowKeys$}
\State $ColGroup, ColId_N \dashleftarrow Metadata$
\State $ReadGroup_N \gets $ \Call{ReadGroups}{$Cols, GolGroup_N$}
\State $Token \gets $ \Call{Token}{$PartitionKey$}
\State $CandidatePages \gets $\Call{Lookup}{$PartitionIndex, Token$}
\For{$Page \gets CandidatePages$}
 \State $Count \dashleftarrow DataFile_{0}[Page]$
 \State $TokenBase \gets Page +  $ \Call{SizeOf}{\textsc{$Count$}}
 \State $OffsetsBase \gets TokenBase + Count \times $\Call{SizeOf}{\textsc{Token}}
 \For{$i \gets $ \Call{Find}{$DataFile_0[TokenBase \dots OffsetBase), Token$}
  \State $OffsetPosition \gets OffsetsBase + i \times $ \Call{SizeOf}{\textsc{Offset}}
  \State $Offset \dashleftarrow DataFile_{0}[OffsetPosition]$
  \State $PartitionEpoch, CandidateKey, Pos_N \dashleftarrow DataFile_{0}[Offset]$
  \If{$PartitionKey = CandidateKey$} 
   \State $Offset \gets Offset + $ \Call{SizeOf}{$\langle PartitionEpoch, CandidateKey, Pos_N \rangle$}
   \State $RowIndex \gets DataFile_0[Offset]$
   \State \Return \Call{Build}{$RowKeys, PartitionEpoch, ReadGroup_N, ColId_N, RowIndex, Pos_N, DataFile_N$}
  \EndIf
 \EndFor
\EndFor
\Return \textbf{nil}
\EndFunction
\Statex
\Procedure{Scan}{$StartToken, EndToken, Cols, RowKeys, Out$}
\State $ColGroup, ColId_N \dashleftarrow Metadata$
\State $Read_N \gets $ \Call{ReadGroups}{$Cols, GolGroup_N$}
\For{$Page \gets $\Call{Range}{$PartitionIndex, Start, End$}}
 \State $Count \dashleftarrow DataFile_{0}[Page]$
 \State $TokenBase \gets Page +  $ \Call{SizeOf}{\textsc{$Count$}}
 \State $OffsetsBase \gets TokenBase + Count \times $\Call{SizeOf}{\textsc{Token}}
 \For{$0 \leq i < Count$}}
  \State $TokenPosition \gets BasePos + i \times $ \Call{SizeOf}{\textsc{Token}}
  \State $Token \gets DataFile_0[TokenPosition]$
  \If{$StartToken \leq Token < EndToken$}
   \State $OffsetPosition \gets OffsetsBase + i \times $ \Call{SizeOf}{\textsc{Offset}}
   \State $Offset \dashleftarrow DataFile_{0}[OffsetPosition]$
   \State $PartitionEpoch, PartitionKey, Pos_N, RowIndex \dashleftarrow DataFile_{0}[Offset]$
   \State $Buffer \Leftarrow $ \Call{Build}{$RowKeys, PartitionEpoch, ReadGroup_N, ColId_N, RowIndex, Pos_N, DataFile_N$}
   \State $BufferSize \gets $ \Call{SizeOf}{$Buffer$}
   \If{$BufferSize \geq BufferLimit$}
    \State $Out \Leftarrow \min(Buffer)$
    \State $Buffer \gets Buffer / {\min(Buffer)}$
   \EndIf
  \EndIf
 \EndFor
\EndFor
\EndProcedure
\Statex
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\scriptsize
\caption{Reading - Helper Methods}
\begin{algorithmic}[2]
\Function{ReadGroups}{$Cols, ColGroup$}
\State $ReadGroup_{n} \gets \{ c | \forall c \in Cols, ColGroup(c) = n\}$
\While{$ \exists n \cdot Read_{n} \subseteq (Read_{N} \setminus ReadGroup_{n}) $}
 \State $ReadGroup_{n} \gets \emptyset$
\EndWhile
\While{$ \exists n,m \in ReadGroup_{N} \cdot Read_{n} \cap ReadGroup_{m} \neq \emptyset $}
 \State $ReadGroup_{n} \gets ReadGroup_{n} \setminus Read_{m}$
\EndWhile
\Return $ReadGroup_N$
\EndFunction
\Statex

\Function{Build}{$RowKeys, PartitionEpoch, ReadGroup_N, ColId_N, Pos_N, DataFile_N$}
\State $Partition \gets $ \Call{New}{}
\State $ValueStore_n \Leftarrow \langle DataFile_n[Pos_n] \rangle mid \forall n \in N$
\For{$RowKey \gets RowKeys$}
 \State $Row \gets $ \Call{New}{}
 \State $Pos_N, EpochDelta \gets$ \Call{Read}{$RowIndex, RowKey$}
 \State $RowEpoch \gets PartitionEpoch + EpochDelta$
 \For{$n \gets N : ReadGroup_n \neq \emptyset$}
  \State $Row \Leftarrow $ \Call{Read}{$ValueStore, ColId_n(ReadGroup_n), Pos_n, RowEpoch$}
 \EndFor
 \State $Partition \Leftarrow Row$
\EndFor
\Return $Partition$
\EndFunction

\Statex
\Function{Find}{$DataFile_0[Start \dots End], Token$}
\State \Return All indexes in the range storing the same value as $Token$
\EndFunction
\Statex
\Function{Lookup}{$PartitionIndex, Token$}
\State \Return All positions possibly containing $PartitionKey$
\EndFunction
\Statex
\Function{Range}{$PartitionIndex, StartToken, EndToken$}
\State \Return ${Page \mid \forall (Token \mapsto Page) \in PartitionIndex, StartToken <= Key < EndToken }$
\EndFunction
\Statex
\Function{Read}{$RowIndex, RowKey$}
\State \Return $\langle$Offsets in each $DataFile$ at which $ValueStore$ persisted $RowKey$, $RowEpoch \rangle$
\EndFunction
\Statex
\Function{Read}{$ValueStore, ColIds, Pos, RowEpoch$}
\State \Return Values for the $ColIds$ persisted by $ValueStore$ at relative $Pos$
\EndFunction

\end{algorithmic}
\end{algorithm}

\clearpage
\paragraph{Notes}
\small
\begin{itemize}
  \item Large Partitions\\[2pt]
    This pseudocode implementation has ignored certain complexities of dealing with very large
    partitions, and assumes we can assemble the entire partition in memory. This was to keep
    the complexity to a minimum. To solve this problem,
    partitions with a $RowIndex$ spread across many pages can be filtered into a separate
    file, with $Group_0$ being shifted to $Group_{-1}$, so that no values are stored alongside
    the $RowIndex$. Thus we do not need to know its length, since there is no $ValueStore_0$ to locate.
    Since only one partition will be serialized we also do not need to know the position of the next $RowIndex$.
  \item Dynamic Block Size\\[2pt]
    Given the majority of Cassandra deployments utilise hash partitioning, we will approach optimal 
    behaviour the fewer partitions we retrieve in a single IOP, especially on SSDs. Whether or not 
    we change the block size intra file can be decided later, but at a minimum each file should 
    select a block size optimal for the data distribution being written to it.
\end{itemize}

\section{Component Variants}

This section will discuss some of the options for component implementations, their limitations and
in which scenarios they might help. A tentative delivery selection and order will also be proposed.
\\
\paragraph{PartitionIndex}
\paragraph{}
The partition index most likely needs just two implementations, although it may be that subtle variations
of each are tried over time. The implementations depend on the nature of partitioning scheme, which
fall into two categories: Ordered and Hashed. So we need an ordered map, and a hash map implementation.
\begin{itemize}
  \item $Ordered$\\[2pt]
    The obvious choice is a B\textsuperscript{+}-tree, although since our data is static some alternatives are viable.
    Since this partitioning scheme is uncommon, however, the labour may not be worth investing in a new approach,
    and a simplification of the current scheme may be sensible. The only difference is that each page has its own
    mini index, so we only need to store the start and end tokens for each page. This translates roughly to 
    the sstable representing the bottom two levels of a B\textsuperscript{+}-tree, with the higher 
    of the two remaining resident in memory as our index.
  \item $Hashed$\\[2pt]
    There are a multitude of ways the hash layout can be exploited. Here we will outline
    a method that in many cases should permit single IOPs for each read while occupying very little 
    resident memory, however the efficiency of the technique will vary based on partition size distributions.
    The principle is quite simple: given a good quality hash function (and in conjunction with 
    \href{https://issues.apache.org/jira/browse/CASSANDRA-6696}{CASSANDRA-6696}), if the partitions are
    all of equal size we can know exactly where to seek for a record that \textem{does} exist: if there
    are $N$ pages, representing $StartToken$ and $EndToken$, then we should look in page $N \times \dfrac{Token - StartToken}{EndToken - StartToken}$
    
    However, not all partitions will be the same size. So we perform this calculation for each stored token
    and compare the value against the truth. We build a tree of ranges to known inaccuracy, and we traverse
    this tree until the inaccuracy
    would be from the truth for all tokens we know to be in the range
    
    The system proposed here is to exploit the entropy of our hash function and the static nature of the
    data: by simply writing the hashes in a file in order with a known size (i.e. the hash size and the
    address size), we can locate the page that should contain the hash we want to lookup simply by assuming 
    a uniform distribution. I 
    this should generally be a valid assumption, although a cryptographically secure hash would enforce
    this more concretely. To mitigate deviance from this assumption, and to limit the impact of determined
    attackers, we can
    We can, at write time, also construct a list of boundaries for each
  \item Out of Order Records\\[2pt]
    In both cases out-of-order persistence is permitted, and to support this a generalized pre-index
    may be useful, in which we store the few tokens we have permitted to be re-ordered. Lookups hit
    this index first, and on finding no results fallback to the normal index.
\begin{algorithm}
\scriptsize
\caption{Hash Partition Lookup}
\begin{algorithmic}[2]
\Function{Lookup}{$PartitionIndex, PartitionKey$}
\State $Hash \gets $ \Call{Hash}{$PartitionKey$}
\State $Position$
\State \Return All positions possibly containing $PartitionKey$
\EndFunction

\end{algorithmic}
\end{algorithm}
    
\end{itemize}

\end{document}
