\documentclass[fleqn]{article}

\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{xpatch}
\usepackage[noend]{algpseudocode}

\makeatletter
\makeatother

\title{Cassandra Storage Format\\Proposal for C* 3.1\small\\Version 2}
\author{Benedict Elliott Smith}
\date{January 2015}

\begin{document}

\maketitle

\begin{abstract}
This document is a draft proposal for a generalised replacement of the sstable format used in Cassandra.
The goal is to deliver a framework building on top of a number of abstracted components, with which
any efficient sstable representation can be encapsulated. It is designed to directly align with modern CQL
data modelling, however it should also support thrift models at least as efficiently as the current state of art.
\end{abstract}

\small

\section{Components}
First we will outline the abstract definitions of the components we will use in this framework, the
expectations we have of them, and the manner in which they will be used.
\subparagraph{$PartitionIndex$}
\subparagraph{}
    One per sstable. Performs a translation from \textsc{Token}$(PartitionKey)$ to a set of addresses 
    in a $DataFile$ that may contain data for the key. This translation is permitted to yield false positives, 
    but should expect to produce either zero or one result on average, with no false negatives. Multiple keys 
    may be stored at the same address in the $DataFile$, which can be exploited by an implementation to produce a 
    more compact mapping. These keys will generally be adjacent, but some may be out-of-order due to page 
    packing (see Extension Two). Persistence is managed by the implementation itself.
    \\
    \begin{algorithmic}[2]
    \scriptsize
    \Function{Lookup}{$PartitionIndex, Token$}
    \Statex \Return All positions possibly containing $PartitionKey$
    \EndFunction
    \Statex{\bf function} \textsc{Range} ({$PartitionIndex, StartToken, EndToken$})
    \Statex \Return ${Page \mid \forall (Token \mapsto Page) \in PartitionIndex, StartToken <= Token < EndToken }$
    \end{algorithmic}

\clearpage
\subparagraph{$RowIndex$}
\subparagraph{}
    One per partition. Located by a $PartitionIndex$ lookup, it stores a translation from \textit{Clustering Columns}
    (here on referred to as $RowKey$) to both a positional key into the $ValueStore$ and a $RowEpoch$. It also
    stores range tombstones. It will be necessary to support a variable number of clustering columns so that
    range deletes will be supported when the functionality is delivered. This may also be used to encode 
    collection keys. 
    \\\\
    The contract with the framework is to manage 
    a sequence of bytes that contain its data, indexed from zero. The framework manages the actual persistence 
    of this byte sequence, and the location of the zero position. In actual implementation it may be helpful 
    to abstract this sequence of bytes to a sequence of pages to help efficient indexing of large partitions, 
    but we leave this complexity out of this document.
    \\
    \begin{algorithmic}[2]
    \scriptsize
    \Function{Read}{$RowIndex, RowKey$}
    \Statex \Return $\langle RowEpoch$, Positional key in $ValueStore$ for $RowKey\rangle \mid DeletedTimestamp$
    \EndFunction
    \end{algorithmic}

\subparagraph{$ValueStore$}
\subparagraph{}
    One per partition. A sequence of bytes that combines the position retrieved from the $RowIndex$ with the 
    columns to be queried and the $RowEpoch$ to return cell values. We assume here that, prior to insertion of 
    a new row, a property $Pos$ can be queried, returning the key to store in $RowIndex$ 
    for future retrieval of the row we are about to insert.
    Like $RowIndex$, it is treated as a zero-indexed sequence of bytes, the persistence of which is dealt 
    with by the framework.
    \\
    \begin{algorithmic}[2]
    \scriptsize
    \Function{Read}{$ValueStore, ColIds, Pos, RowEpoch$}
    \Statex \Return Cells for the $ColIds$ persisted by $ValueStore$ at positional key $Pos$
    \EndFunction
    \end{algorithmic}

\paragraph{}
\section{Implementation Details}
\small
The components defined in the prior section should compose reasonably naturally, as far as intuition
is concerned. Here we attempt to define it more concretely, with some both descriptive and mathematically
explicit pseudocode describing how they can be stitched together to form a generalised storage framework.
\\
\subsection{Notation Key}
\paragraph{}
\begin{tabular}{l l}
$X \Leftarrow Y$ & Write/Add/Append $Y$ To $X$\\
$X \dashleftarrow Y$ & Read $X$ From $Y$\\
$X \gets Y$ & Set $X$ to $Y$\\
$\langle A,B \rangle$ & A tuple containing the values $A$ and $B$\\
$\{Item \mid Constraint\}$ & The set of all $Item$ produced by $Constraint$\\
$S_N$ & $\equiv \{S_n \mid n \in N\}$\\
$Stmt_x \mid \forall x \in X$ & Execute statement $S$ for each $x$ in $X$\\
$\lvert X \rvert$ & $\equiv \mathbf{card}(X)$, i.e. the number of elements in $X$\\
\end{tabular}

\clearpage
\subsection{Core Algorithm}
\paragraph{Definitions}
\begin{itemize}
  \item $PartitionEpoch$, $RowEpoch$\\[2pt]
    Against each tier of our structure we will encode the minimum timestamp found in the next
    tier, so that each may be encoded more efficiently. The expectation is that this will permit
    many cells to persist their timestamp using only a few bytes, a single bit, or no data at all
  \item $DataFile$\\[2pt]
    A persistent byte storage medium, supporting append and buffered sequential and random reads
  \item $ColId$\\[2pt]
    A translation from column to a unique integer id in the range $[0..N)$
\end{itemize}

\begin{algorithm}
\scriptsize
\caption{Core Algorithm - Descriptive}
\begin{algorithmic}[1]
\Procedure{Write}{$Partitions$}
\State Map each column to a unique integer id in the range $[0..N)$
\ForAll{Partitions} 
 \State Extract the $PartitionEpoch$
 \ForAll{Rows in the Partition}
   \State Extract the $RowEpoch$, subtracting it from each cell
   \State Add the $ValueStore$ position/key and $RowEpoch$ to $RowIndex$
   \State Add the cells to the $ValueStore$
 \EndFor
 \State Add everything to our $Buffer$
 \If{$Buffer$ is full}
   \State Add the partition keys to our $PartitionIndex$, mapping to our page index
   \State Write the tokens in our buffer (fixed-width)
   \State Write each partition's offset within the page (fixed-width)
   \State Write each partition's data
 \EndIf
\EndFor
\EndProcedure
\Statex
\Function{Read}{$PartitionKey, Cols, RowKeys$}
\State Map $Cols$ to $ColIds$ using our metadata
\ForAll{Pages associated with \textsc{Token}($PartitionKey$) in $PartitionIndex$}
 \State Binary search for \textsc{Token}($PartitionKey$)
 \ForAll{Matching tokens}
  \State Find and seek to the associated data offset
  \If{The key matches $PartitionKey$} 
   \State Read the $PartitionEpoch$
   \State Initialize $RowIndex$ and $ValueStore$ to their starting positions
   \ForAll{Matching rows in $RowIndex$}
     \If{Is $DeletedTimestamp$}
      \State Add the range tombstone to our response
     \Else
      \State Read the associated $RowEpoch$ and $Pos$
      \State Provide these and $ColIds$ to $ValueStore$
      \State Add the result to our response
     \EndIf
   \EndFor
   \Return Constructed response
  \EndIf
 \EndFor
\EndFor
\Return \textbf{nil}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\scriptsize
\caption{Core Algorithm - Precise}
\begin{algorithmic}[1]
\Procedure{Write}{$Partitions$}
\State $PartitionIndex, DataFile \gets $ \Call{New}{}
\State $ColId \gets \{c \mapsto i \mid \forall c, d \in Columns, d < c \implies d \mapsto j \wedge j < i \}$
\State $Metadata \Leftarrow ColId$
\Statex
\For{$p \gets Partitions$} 
 \State $RowIndex, ValueStore \gets $ \Call{New}{}
 \State $PartitionEpoch \gets \min \{v.timestamp \mid v \in r.Values, r \in p.Rows\}$
 \Statex
 \For{$r \gets p.Rows$}
   \State $RowEpoch \gets \min \{v.timestamp \mid v \in r.Values\}$
   \State $EpochDelta \gets RowEpoch - PartitionEpoch$
   \State $RowIndex \Leftarrow \langle r.RowKey \mapsto \langle EpochDelta, ValueStore.Pos \rangle \rangle $ 
   \State $Values \gets \{ ColId(c) \mapsto r.Values(c) \mid c \in Columns \}$
   \State $v.Time \gets v.Time - RowEpoch \mid \forall v \in \mathbf{ran}(Values)$
   \State $ ValueStore \Leftarrow Values $
 \EndFor
\Statex
 \State $Buffer \Leftarrow \langle p.PartitionKey, PartitionEpoch, RowIndex, ValueStore, DataFile.Pos \rangle$
 \State $Size \gets DataFile.pos - \min\limits_{\forall b \in Buffer} b.Pos $
 \If {$Size \geq PageSize$}
   \State \Call{Flush}{$Flush, PartitionIndex, DataFile$}
   \State $Buffer \gets $ \Call{New}{}
 \EndIf
\EndFor
\State \Call{Flush}{$Buffer, PartitionIndex, DataFile$}
\EndProcedure
\Statex
\Procedure{Flush}{$Buffer, PartitionIndex, DataFile$}
   \State $PartitionIndex \Leftarrow \langle $\Call{Token}{$PartitionKey$}$ \mapsto DataFile.Pos \rangle \mid \forall PartitionKey \in Buffer$
   \State $DataFile \Leftarrow \lvert Buffer \rvert$
   \State $DataFile \Leftarrow $ \Call{Token}{$PartitionKey$} $ \mid \forall \langle PartitionKey, \_,\_,\_\rangle \in Buffer \rangle$
   \State $Offset \gets 0$
   \For{$\langle PartitionKey, \_, \_, \_ \rangle \gets Buffer$}
     \State $DataFile \Leftarrow Offset$
     \State $Offset \gets Offset + $ \Call{SizeOf}{$\langle PartitionKey, RowIndex, ValueStore,$\textsc{Epoch}$,$\{\textsc{Address}\}$\rangle$}
   \EndFor
   \For{$\langle PartitionKey, PartitionEpoch, RowIndex, ValueStore \rangle \gets Buffer$}
    \State $DataFile \Leftarrow \langle PartitionEpoch, PartitionKey \rangle$
    \State $DataFile \Leftarrow DataFile.Pos + $ \Call{SizeOf}{$RowIndex$} $ + $ \Call{SizeOf}{\{\textsc{Address}\}}
    \State $DataFile \Leftarrow RowIndex$
    \State $DataFile \Leftarrow ValueStore$
   \EndFor
\EndProcedure
\Statex
\Function{Read}{$PartitionKey, Cols, RowKeys$}
\State $ColGroup, ColId \dashleftarrow Metadata$
\State $Token \gets $ \Call{Token}{$PartitionKey$}
\State $CandidatePages \gets $\Call{Lookup}{$PartitionIndex, Token$}
\For{$Page \gets CandidatePages$}
 \State $Count \dashleftarrow DataFile[Page]$
 \State $TokenBase \gets Page +  $ \Call{SizeOf}{\textsc{$Count$}}
 \State $OffsetsBase \gets TokenBase + Count \times $\Call{SizeOf}{\textsc{Token}}
 \For{$i \gets $ \Call{Find}{$DataFile[TokenBase \dots OffsetBase), Token$}}
  \State $OffsetPosition \gets OffsetsBase + i \times $ \Call{SizeOf}{\textsc{Offset}}
  \State $Offset \dashleftarrow DataFile[OffsetPosition]$
  \State $PartitionEpoch, CandidateKey, Pos \dashleftarrow DataFile[Offset]$
  \If{$PartitionKey = CandidateKey$} 
   \State $Offset \gets Offset + $ \Call{SizeOf}{$\langle PartitionEpoch, CandidateKey, Pos \rangle$}
   \State $RowIndex \gets DataFile[Offset]$
   \State \Return \Call{Build}{$RowKeys, PartitionEpoch, ReadGroup, ColId, RowIndex, Pos, DataFile$}
  \EndIf
 \EndFor
\EndFor
\Return \textbf{nil}
\EndFunction
\Statex
\Function{Build}{$RowKeys, PartitionEpoch, ColId, Pos, DataFile$}
\State $Partition \gets $ \Call{New}{}
\State $ValueStore \Leftarrow DataFile[Pos]$
\For{$RowKey \gets RowKeys$}
 \State $RowInfo \gets$ \Call{Read}{$RowIndex, RowKey$}
 \If{$RowInfo \in Timestamp$}
  \State $Partition \Leftarrow RowInfo$
 \Else
  \State $Pos, EpochDelta \gets RowInfo$
  \State $RowEpoch \gets PartitionEpoch + EpochDelta$
  \State $Partition \Leftarrow $ \Call{Read}{$ValueStore, ColId(Columns), Pos, RowEpoch$}
 \EndIf
\EndFor
\Return $Partition$
\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage
\subsection{Extension One - Column Groupings}
\small
When reading data from an sstable, fetching data for any single column necessitates also
reading any columns that were written near-in-time, or have
been compacted together. For tables with many columns this can result in suboptimal query
performance when reading only a subset of the columns.
\\\\
By permitting fields to be grouped for purposes of persistence, we can pack more tightly together data
that may be accessed in isolation within a large partition, permitting range queries over these columns
to incur fewer IOPs. Conversely, rarely accessed fields can be separated into their own group
to avoid their polluting all other common queries.
\\\\ 
Within the framework we also support multiple schemes for value encoding and retrieval, 
which may fit different columns stored in the table differently, or may only support a subset 
of the columns on a table. For example columnar or hybrid value storage may be ideal for
densely populated non-collection timeseries fields. This extension permits us to use these value stores
for such fields  and row-oriented for the remainder; or any other optimal configuration.
\\\\
At the same time, this approach permits us to separate values from the partition index in situations 
we expect many intra-partition range queries, so that the partition indexes can expect better cache occupancy.
It also supports duplicating columns to meet multiple different access patterns with minimal IO, at the expense
of write costs.

\paragraph{Definitions}
\begin{itemize}
  \item (Column) $Group_N$\\[2pt]
    $N$ groupings of columns, each containing a subset of the total columns defined on the table, 
    that will be persisted together in a corresponding $DataFile_N$. By default there will be one 
    group containing all columns, but rarely accessed columns or columnar layouts may be separated 
    into their own group.
  \item $ColId_N$\\[2pt]
    A translation from column to a unique integer id within each $Group_n$
\end{itemize}

\paragraph{Components}
\begin{itemize}
  \item $ValueStore_N$\\[2pt]
    One per $Group_N$, per partition. Otherwise as defined originally.
  \item $DataFile_N$\\[2pt]
    One per $Group_N$, per partition, storing the associated $ValueStore_N$, with $DataFile_0$ 
    storing our $RowIndex$ and partition-wide data.
\end{itemize}

\begin{algorithm}
\scriptsize
\caption{Column Groupings}
\begin{algorithmic}[1]
\Procedure{Write}{$Partitions, Group_N$}
\State $PartitionIndex, Buffer \gets $ \Call{New}{}
\State $DataFile_{n} \gets $ \Call{New}{} $\mid \forall n \in N$
\Statex
\State $ColGroup \gets \{c \mapsto n \mid \forall c \in Group_{n}, \forall n \in N \}$
\State $ColId_{n} \gets \{c \mapsto i \mid \forall c, d \in G, d < c \implies d \mapsto j \wedge j < i \} \mid \forall n \in N, G \equiv Group_{n}$
\State $Metadata \Leftarrow ColGroup$
\State $Metadata \Leftarrow ColId$
\Statex
\For{$p \gets Partitions$} 
 \State $RowIndex \gets $ \Call{New}{}
 \State $ValueStore_{n} \gets $ \Call{New}{} $ \mid \forall n \in N$
 \State $PartitionEpoch \gets \min \{v.timestamp \mid v \in r.Values, r \in p.Rows\}$
 \Statex
 \For{$r \gets p.Rows$}
   \State $RowEpoch \gets \min \{v.timestamp \mid v \in r.Values\}$
   \State $EpochDelta \gets RowEpoch - PartitionEpoch$
   \State $RowIndex \Leftarrow \langle r.RowKey \mapsto \langle EpochDelta, \{ v.Pos \mid v \in ValueStore_{N} \}\rangle \rangle $ 
   \State $Values_{n} \gets \{ ColId(c) \mapsto r.Values(c) \mid c \in Group_{n} \} \mid \forall n \in N$
   \State $v.Time \gets v.Time - RowEpoch \mid \forall v \in \mathbf{ran}(Values_{n}), \forall n \in N$
   \State $ ValueStore_{n} \Leftarrow Values_{N} $
 \EndFor
\Statex
 \State $Pos_{n} \gets DataFile_{n}.Pos | \forall n \in N$
 \State $Buffer \Leftarrow \langle p.PartitionKey, PartitionEpoch, RowIndex, ValueStore_{N}, Pos_{N} \rangle$
 \State $Size \gets \sum\limits_{n=0}^{N} (DataFile_{n}.pos - \min\limits_{\forall b \in Buffer} b.Pos_{n}) $
 \If {$Size \geq N \times PageSize$}
   \State $Flush \gets$ \Call{Select}{$Buffer$}
   \State \Call{Flush}{$Flush, PartitionIndex, DataFile_{N}$}
   \State $Buffer \gets $ \Call{new}{}
 \EndIf
\EndFor
\State \Call{Flush}{$Buffer, PartitionIndex, DataFile_{N}$}
\EndProcedure
\Statex
\Procedure{Flush}{$Buffer, PartitionIndex, DataFile_{N}$}
   \State $PartitionIndex \Leftarrow \langle $\Call{Token}{$PartitionKey$}$ \mapsto DataFile_{0}.Pos \rangle \mid \forall PartitionKey \in Buffer$
   \State $DataFile_{0} \Leftarrow \lvert Buffer \rvert$
   \State $DataFile_{0} \Leftarrow $ \Call{Token}{$PartitionKey$} $ \mid \forall \langle PartitionKey, \_,\_,\_\rangle \in Buffer \rangle$
   \State $Offset \gets 0$
   \For{$\langle PartitionKey, \_, \_, \_ \rangle \gets Buffer$}
     \State $DataFile_0 \Leftarrow Offset$
     \State $Offset \gets Offset + $ \Call{SizeOf}{$\langle PartitionKey, RowIndex, ValueStore_0, $ \textsc{Epoch}$,$ \{\textsc{Address}$^N$\}$\rangle$}
   \EndFor
   \For{$\langle PartitionKey, PartitionEpoch, RowIndex, ValueStore_{N} \rangle \gets Buffer$}
    \State $DataFile_{0} \Leftarrow \langle PartitionEpoch, PartitionKey \rangle$
    \State $Pos_{n} \gets DataFile_{n}.Pos \mid \forall n \in N$
    \State $Pos_{0} \gets Pos_0 + $ \Call{SizeOf}{$RowIndex$} $ + $ \Call{SizeOf}{\{\textsc{Address$^N$}\}}
    \State $DataFile_{0} \Leftarrow Pos_{N}$
    \State $DataFile_{0} \Leftarrow RowIndex$
    \State $DataFile_{n} \Leftarrow ValueStore_{n} \mid \forall n \in N$
    \State Pad $DataFile_{n}$ to a page boundary $\mid \forall n \in N$
   \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\setcounter{algorithm}{2}
\scriptsize
\caption{Column Groupings (contd.)}
\begin{algorithmic}[1]
\Function{Read}{$PartitionKey, Cols, RowKeys$}
\State $ColGroup, ColId_N \dashleftarrow Metadata$
\State $ReadGroup_N \gets $ \Call{ReadGroups}{$Cols, GolGroup_N$}
\State $Token \gets $ \Call{Token}{$PartitionKey$}
\State $CandidatePages \gets $\Call{Lookup}{$PartitionIndex, Token$}
\For{$Page \gets CandidatePages$}
 \State $Count \dashleftarrow DataFile_{0}[Page]$
 \State $TokenBase \gets Page +  $ \Call{SizeOf}{\textsc{$Count$}}
 \State $OffsetsBase \gets TokenBase + Count \times $\Call{SizeOf}{\textsc{Token}}
 \For{$i \gets $ \Call{Find}{$DataFile_0[TokenBase \dots OffsetBase), Token$}}
  \State $OffsetPosition \gets OffsetsBase + i \times $ \Call{SizeOf}{\textsc{Offset}}
  \State $Offset \dashleftarrow DataFile_{0}[OffsetPosition]$
  \State $PartitionEpoch, CandidateKey, Pos_N \dashleftarrow DataFile_{0}[Offset]$
  \If{$PartitionKey = CandidateKey$} 
   \State $Offset \gets Offset + $ \Call{SizeOf}{$\langle PartitionEpoch, CandidateKey, Pos_N \rangle$}
   \State $RowIndex \gets DataFile_0[Offset]$
   \State \Return \Call{Build}{$RowKeys, PartitionEpoch, ReadGroup_N, ColId_N, RowIndex, Pos_N, DataFile_N$}
  \EndIf
 \EndFor
\EndFor
\Return \textbf{nil}
\EndFunction
\Statex
\Function{Build}{$RowKeys, PartitionEpoch, ReadGroup_N, ColId_N, Pos_N, DataFile_N$}
\State $Partition \gets $ \Call{New}{}
\State $ValueStore_n \Leftarrow \langle DataFile_n[Pos_n] \rangle \mid \forall n \in N$
\For{$RowKey \gets RowKeys$}
 \State $Row \gets $ \Call{New}{}
 \State $Pos_N, EpochDelta \gets$ \Call{Read}{$RowIndex, RowKey$}
 \State $RowEpoch \gets PartitionEpoch + EpochDelta$
 \For{$n \gets N : ReadGroup_n \neq \emptyset$}
  \State $Row \Leftarrow $ \Call{Read}{$ValueStore, ColId_n(ReadGroup_n), Pos_n, RowEpoch$}
 \EndFor
 \State $Partition \Leftarrow Row$
\EndFor
\Return $Partition$
\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage
\subsection{Extension Two - Partition Reordering}
\small
    An sstable represents an ordered collection of partitions, however this doesn't
    require the data be in order on disk. By permitting records to appear out-of-order
    we can reduce the number of records that cross a page boundary, reducing the number of IOPs 
    needed per query. This should result in increased throughput and reduced maximum latencies. 
    For linear scan performance we need ordered traversal, however this only necessitates
    records occurring \emph{almost} in order. We define a $BufferLimit$ which bounds the amount 
    of buffer space any reader can be required to have to produce results in order.
    Flushing enforces this by buffering some multiple $K$ of $BufferLimit$, and selecting 
    from this buffer a well packed page that favours records earlier in the stream, while leaving a 
    good distribution of records to help pack future pages.

\begin{algorithm}
\scriptsize
\caption{Partition Reordering}
\begin{algorithmic}[1]
\Procedure{Write}{$Partitions, Group_N$}
\State $\dots$
\Statex
\For{$p \gets Partitions$} 
 \State $\dots$
 \State $Distance \gets \sum\limits_{n=0}^{N} (DataFile_{n}.pos - \min\limits_{\forall b \in Buffer} b.Pos_{n}) $
 \If {$Distance \geq K \times BufferLimit$}
   \State $Flush \gets$ \Call{Select}{$Buffer$}
   \State \Call{Flush}{$Flush, PartitionIndex, DataFile_{N}$}
   \State $Buffer \gets Buffer \setminus Flush$
 \EndIf
\EndFor
\EndProcedure
\Statex
\Function{Select}{$Buffer$}
\State $Selection \gets$ ?
\Ensure $Selection \subseteq Buffer$
\Ensure $s \in Selection \cdot s.Pos = \min\limits_{\forall b \in Buffer} b.Pos_{n}$
\Ensure ($\exists k \cdot \sum\limits_{s \in Selection} s.Size_{n} \approx k \times PageSize) \mid \forall n \in N $
\Ensure Future calls to \Call{Select}{} can also meet criteria
\State \Return $Selection$
\EndFunction
\Statex
\Procedure{Scan}{$StartToken, EndToken, Cols, RowKeys, Out$}
\State $ColGroup, ColId_N \dashleftarrow Metadata$
\State $Read_N \gets $ \Call{ReadGroups}{$Cols, GolGroup_N$}
\For{$Page \gets $\Call{Range}{$PartitionIndex, Start, End$}}
 \State $Count \dashleftarrow DataFile_{0}[Page]$
 \State $TokenBase \gets Page +  $ \Call{SizeOf}{\textsc{$Count$}}
 \State $OffsetsBase \gets TokenBase + Count \times $\Call{SizeOf}{\textsc{Token}}
 \For{$0 \leq i < Count$}
  \State $TokenPosition \gets BasePos + i \times $ \Call{SizeOf}{\textsc{Token}}
  \State $Token \gets DataFile_0[TokenPosition]$
  \If{$StartToken \leq Token < EndToken$}
   \State $OffsetPosition \gets OffsetsBase + i \times $ \Call{SizeOf}{\textsc{Offset}}
   \State $Offset \dashleftarrow DataFile_{0}[OffsetPosition]$
   \State $PartitionEpoch, PartitionKey, Pos_N, RowIndex \dashleftarrow DataFile_{0}[Offset]$
   \State $Buffer \Leftarrow $ \Call{Build}{$RowKeys, PartitionEpoch, ReadGroup_N, ColId_N, RowIndex, Pos_N, DataFile_N$}
   \State $BufferSize \gets $ \Call{SizeOf}{$Buffer$}
   \If{$BufferSize \geq BufferLimit$}
    \State $Out \Leftarrow \min(Buffer)$
    \State $Buffer \gets Buffer / {\min(Buffer)}$
   \EndIf
  \EndIf
 \EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\clearpage
\subsection{Extension Two Alternative - Partition Redirection}
\small
  An alternative to the approach just outlined, but achieving the same ends, is 
  producing multiple sstables at flush time, of logarithmically smaller size. 
  When flushing we write the contents of our buffer in order, but omit a subset that
  permit us to pack our pages better. Any omitted are siphoned into their own buffer for a smaller
  sstable, on which the logic is repeated (until returns are minimal). The advantage of this approach
  is it may combine well with techniques we will outline later for managing hash partition indexes,
  and does not require partition index implementations to cope with out-of-order records.
\\\\
  One slight further variant on this approach is to write all of these pages to the same sstable,
  persisting somewhere which subset of pages contain "out-of-order" records - really a parallel order.
  This could meet the constraints imposed by \textit{Partition Reordering} 
  by ensuring at most \tiny{$\dfrac{BufferLimit}{PageSize}$}\small parallel-order pages must be retained 
  by any scanner. This also avoids $PartitionIndex$ support for out-of-order records, 
  but requires a separate bloom filter and partition index for each parallel order.

\subsection{Notes}
\small
\paragraph{Large Partitions}
\paragraph{}
    These pseudocode implementations have ignored certain complexities of dealing with very large
    partitions, and assumed we can assemble the entire partition in memory. This was to keep
    the complexity of our pseudocode to a minimum. To solve this problem, partitions with a $RowIndex$ 
    spread across many pages can be filtered into a separate file, with $Group_0$ being shifted to 
    $Group_{-1}$, so that no values are stored alongside the $RowIndex$. Thus we do not need to know 
    its length, since there is no $ValueStore_0$ to locate.
    Since only one partition will be serialized we also do not need to know the position of the next $RowIndex$.
    \\\\
    For most sstables, a majority of records will fall one side of this barrier, so there should not be a
    significant increase in sstable count. Since this will only be performed for large partitions spread over
    multiple pages, there will also be no IO penalty.
    \\\\
    Alternatively, if providing a page-oriented byte layer abstraction, on encountering an
    oversized record we can immediately flush the contents of our write buffer, then interleave 
    serialization of $RowIndex$ and $ValueStore_0$ pages for the large partition.
\paragraph{$RowKey$ Abstraction}
\paragraph{}
    For simplicity of presentation we have assumed a set of $RowKey$ will be provided for any query.
    In reality range slices will be common. Behaviour is not meaningfully different, and clarity 
    is improved by making this assumption for presentation.
\paragraph{Dynamic Block Size}
\paragraph{}
    At a minimum each file should select a block size optimal for the storage medium and the data distribution 
    being written to it. For SSDs this should mean a block size just large enough to fit a single partition, 
    or just its row index, if such a block size exists; otherwise, a 4Kb block size (also our minimum block size).
    For spinning disks, a larger block size can be used unconditionally; say 32-64Kb, as is currently the case.
    We should consider/explore using a dynamic block size within each file.
\paragraph{Decoupling of $ValueStore_N$ from $DataFile_N$}
\paragraph{}
    We have assumed each $ValueStore_n$ has its own $DataFile_n$, but there is no requirement that this holds true,
    nor that a selection of alike $ValueStore$ could not be grouped and indexed by the same $RowIndex$ entry. Pure 
    column oriented value stores could all be grouped in this manner. These are simple additions that only 
    complicate the pseudocode unnecessarily.
\paragraph{Collections}
\paragraph{}
    Collections have not been dealt with at all directly in this document. We will touch on them briefly here,
    and outline some of the ways they can be dealt with.
    \begin{enumerate}
      \item{$RowIndex$ Presence}\\[2pt]
        Perhaps the most obvious approach would be to continue the current paradigm of treating the collection key
        as a suffix of the clustering data. While $RowIndex$ implementations will need to support variable length
        prefixes of clustering data, supporting variable length suffixes may be an unwelcome burden, and this
        would also require some special treatment in row-oriented $ValueStore$ to reduce the amount of row metadata.
      \item{Direct Value Encoding}\\[2pt]
        Alternatively, we could persist the entire collection as a dynamic width field in the row. For small
        collections this is likely optimal in both performance and implementation complexity. The data could
        be stored using a $RowIndex$ implementation whose zero index is the dynamic column offset.
      \item{Indirect Value Encoding}\\[2pt]
        If the collection is large, we could instead store a fixed offset to a page in the $DataFile_0$, or some
        special spill-over $DataFile_S$ where the contents are stored. In implementation there would be little
        difference between this and \textit{Direct Value Encoding}; both can utilise a $RowIndex$ implementation,
        and only vary the locality of storage.
    \end{enumerate}

\section{Component Variants}
Here we will outline the main variants for each component, and some of the details of their implementation.

\subsection{Partition Index}
The partition index most likely needs just two implementations, although it may be that subtle variations
of each are tried over time. The implementations depend on the nature of the partitioning scheme, which
fall into two categories: Ordered and Hashed.
\paragraph{Ordered}
\paragraph{}
    The obvious choice is a B\textsuperscript{+}-tree, although since our data is static some alternatives are viable.
    Since this partitioning scheme is uncommon, however, it may not be worth investing labour in a major new 
    approach, and a simplification of the current scheme may be sensible, with some modifications to match our
    new layout.
    \\\\
    Since each page has its own internal micro index, our macro index need only store the first token occurring 
    in each page. A summary index can be constructed that does the same for the index itself, within which
    we binary search for the index record to consult. This summary can reside in memory, directing us to an 
    index page within which we perform a binary search to find the data page. 
    This translates directly to the sstable representing the 
    bottom three levels of a B\textsuperscript{+}-tree, with the highest remaining resident in memory as our 
    summary, and the middle being the highly cacheable index. For small datasets we could omit the summary level
    and store the entire index in memory, since the required space for the same page size will be significantly 
    lower than the current approach. Only 8 bytes are necessary per page, instead of 16 bytes 
    + \textsc{Len}($PartitionKey$) per partition. If we pack $P$ partitions per page, this translates
    to a minimum fraction of \tiny$\dfrac{1}{2P}$\small, with \tiny$\dfrac{1}{4P}$\small for $UUID$ partition keys.
    
\paragraph{Hashed}
\paragraph{}
    The basic building block for hash partitioning will be the same as \textit{Ordered}, however we can exploit
    the random data distribution to improve the computational complexity of lookup and the space required, 
    both in memory and on disk.
    \\\\
    The basic intuition to exploit is that, given a uniform token distribution and a known start/end 
    for an sstable (\href{https://issues.apache.org/jira/browse/CASSANDRA-6696}{CASSANDRA-6696}) being 
    helpful here), we can calculate the average boundary token that can be expected for 
    each page. This expectation will typically be quite wrong, but the \textit{error} will be distributed 
    over a small range, and this can be exploited in three ways:

    \begin{enumerate}
      \item{\textbf{Hash Indexed Summary}}\\[2pt]
        Instead of performing binary search on the summary, we can predict the correct summary page
        based on the expected distribution of data, at write-time ensuring this is never incorrect
        by more than a predetermined bound. A bounded number of linear probes from the predicted location
        within this page can then be performed to find the desired token boundary, or a binary search
        can be performed over a known maximal range either side of the predicted position.
      \item{\textbf{Hash Indexed Index}}\\[2pt]
        We can eliminate the summary entirely, instead padding the index file so that our expected boundaries 
        for the index file are always correct, requiring exactly one seek into the index file. The amount 
        of padding necessary will typically be small, but if it is large this approach can be abandoned
        in favour of another.
      \item{\textbf{Probabilistic Index Search}}\\[2pt]
        We can alternatively eliminate the summary by predicting a location to begin our index search in, 
        along with an expected range to binary search around, precomputed at write-time.
        This range will contain the desired page mapping with $P(X) \geq Limit$, for some configured $Limit$. 
        If the search fails we expand the area by a similar logic. This yields a predictable performance
        characteristic with fixed memory constraints.
      \item{\textbf{Golomb Encoded Page Boundaries}}\\[2pt]
        We can potentially eliminate both the summary and the index. Instead of encoding our page boundaries 
        as a list of tokens, we can instead encode as the distance between the real token boundary of any
        two pages and the one we predict. The error will be distributed around zero, following a geometric
        distribution, and so can be stored efficiently with Golomb codes with a constant multiplier. 
        This permits tunable accuracy, yielding some $P(X) \geq Limit$ that we will select
        the correct page; if we do not find the correct page, we can be certain the adjacent page is correct,
        so that on average each query performs $2-Limit$ seeks. The level of compression achieved is dependent
        on this $Limit$, and the distribution of the data. These two variables combine to require a scale of 
        multiplier; the multiplier scale in turn determines the range of Golomb codes needed.
        The smaller $Limit$, or better distributed the data, the larger the viable multiplier and hence the 
        smaller the Golomb codes. This technique needs some exploration, but some simple simulation suggests
        viability beyond the theoretical.
        \\\\
        This technique may compose well with \textit{Extension Two - Partition Redirection}, as we could
        select records to persist based both on page density and page boundary.
    \end{enumerate}

\small
\paragraph{Out of Order Records}
\paragraph{}
    If we opt to support out-of-order persistence, a generalized pre-index may be useful, in which we store 
    the few tokens we have permitted to be re-ordered. Lookups hit this index first, and on finding no results
    fallback to the normal index.

\subsection{Row Index}
The Row Index has by far the most scope for variation, so we will only touch briefly on the various
categories here, with a high level overview of their implications. We will mention retrieval and
merge performance, by intuitive description of characteristics only.
\paragraph{Entry Per Cell}
\paragraph{}
    This isn't really possible in the new world, but is worth discussing for comparison since it
    describes the current state-of-art. Here we have the entire clustering prefix repeated for each cell.
    The cost of merging is as suboptimal as possible: 
    \begin{enumerate}
      \item There are $\times \lvert Columns \rvert$ more items to merge than necessary
      \item Shared clustering prefixes must be compared in full, so each comparison is costlier
      \item All rows must be compared; there is no pruning of known disjoint descendant sets 
    \end{enumerate}
\clearpage
\paragraph{Linear Collection}
\paragraph{}
    This is the closest to the current scenario, except that we only repeat the data once per row,
    not per cell. The idea would be to store each complete clustering prefix in sequence,
    and linearly scan for the relevant record. For very large partitions this would not support 
    any true indexing, but it is very simple. For small partitions it would be acceptable, and is
    optimal for single rows. The cost of merging is improved by eliminating (1).
\paragraph{Column Trie, Linear Internal Collections}
\paragraph{}
    The next simplest approach is to split each clustering column into its own set of linear
    sequences of data, so that with multiple clustering columns we do not repeat data present
    in higher tiers. This permits data compression. Merging is also improved by significantly
    reducing the effects of (2) and (3). For partitions whose clustering column tiers are 
    each smaller than a single page this is a fairly optimal approach as binary search 
    can be performed on each tier as you descend.
\paragraph{Column Trie, BTree Internal Collections}
\paragraph{}
    When the trie levels are larger than a page, we need to introduce paging, and to do this a
    BTree makes perfect sense. This is still a very general collection, since any kind of comparison
    can be performed on each BTree item, so it can support all current and custom data types.
    This optimisation permits further improvement to (3) for merging, as well as optimal
    search costs for custom data types and those not supporting binary prefix comparison.
\paragraph{Binary Trie}
\paragraph{}
    If the clustering prefixes can be compared by any binary prefix, a binary trie can be used.
    This permits optimal behaviour for (2) and (3), making merges extremely
    cheap as the number of rows and partitions grow, which is likely to be of significant benefit,
    given how CPU constrained some users are on these costs. This also permits superior
    data compression. There are a number of possible binary trie variants to explore, but this
    warrants a separate document or JIRA ticket to discuss the options.

\subsection{Value Store}
The value store likely has only two major variants, and a hybrid between them: column- and row-oriented.
There is no requirement that a given table subscribe to one or the other, however. Within a single sstable
there can be a mix, with each field grouping selecting its own value persistence approach.
\clearpage
\paragraph{Row Oriented}
\paragraph{}
This most closely resembles the current storage, except that we consider each row a discrete unit
and encode certain information prefixing it to permit indexed access within the row, and to permit
compression of the structural data.
\begin{itemize}
  \item $HasCell$ Bitmap\\[2pt]
    Each column that occurs at least once in the file will have an index in this bitmap, which encodes
    if the column appears in this row, to permit efficient indexing within the row. It may be that some
    columns appear in every row, and these may be encoded in the file metadata to remove them from the
    row bitmaps.
  \item $HasTimestamp$ Bitmap\\[2pt]
    Each column with its bit set in $HasCell$ will have an index in $HasTimestamp$; this 
    will indicate if there is any timestamp offset necessary from $RowEpoch$; a value of zero 
    indicates $RowEpoch$ is enough by itself to construct the cell timestamp.
  \item $HasValue$ Bitmap\\[2pt]
    Each column with its bit set in $HasCell$ will have an index in $HasValue$; this 
    will indicate if there is any actual data associated, or if the "value" is a tombstone.
  \item $HasExpiry$ Bitmap\\[2pt]
    Each column with its bit set in $HasValue$ will have an index in $HasExpiry$; this 
    will indicate if there is an expiry associated with the value.
  \item $HasInfo$ Bitmap\\[2pt]
    A bitmap indicating if $HasCell$, $HasTimestamp$, $HasValue$ or $HasExpiry$ are necessary to 
    encode.
  \item $TimestampLength$\\[2pt]
    If any bits are set in $HasTimestamp$, this will encode how many bytes are needed to encode them.
    Each will be encoded with the same width. This and $HasInfo$ can be encoded in the same byte.
  \item $Timestamps$\\[2pt]
    A fixed-width array of timestamps for each column marked in $HasTimestamp$.
  \item $Expiries$\\[2pt]
    A fixed-width array of expiries for each column marked in $HasExpiry$.
  \item $ColumnWidth : ColId \mapsto \mathbb{N}$\\[2pt]
    A table level property indicating which columns are fixed width, and their widths. A value of $\infty$
    indicates the column is dynamic width.
  \item $ColumnCount$\\[2pt]
    A table level property indicating the number of columns persisted against this value store.
  \item $Values_B$\\[2pt]
    The cells appearing in $HasValue$ whose types permit fixed-width encoding of length $B$ will appear next, 
    so that they may indexed directly without any further information. All columns will appear grouped by
    size, but also in $ColId$ order; we will construct $ColId$ to enforce this at write-time.
  \item $Offsets$\\[2pt]
    The index of any dynamic length fields present in $HasValue$ will follow, so that they may also be 
    accessed directly. Encoded as $Values_{\infty}$.
  \item $DValues$\\[2pt]
    Finally we encode the dynamic length fields themselves.
\end{itemize}

\subparagraph{Notes}
\begin{itemize}
 \item{} It is necessary that our bitmaps support efficient rank operations, i.e. count the number of bits
 less than an index. For small bitmaps ($\leq$ 64 bits) this is trivial, but for larger bitmaps
 it requires a little extra data to implement efficiently. However it may be easiest to encode rows
 as linked-lists of $\leq$ 64 possible columns, since more should be rare.
 \item{} The pseudocode implementation assumes we read all of our bitmaps out of the row, but this is unnecessary
 and done only to aid clarity. Other optimisations with presentation issues are similarly left out.
 \item{} To help read implementation, we select $ColId$ values that correspond to the size of the column, with 
 dynamically sized columns occurring last. This permits us to walk the columns that are being queried in the order
 the data is stored on disk, and to perform fewer calculations for indexing into it. 
\end{itemize}

\begin{algorithm}
\scriptsize
\caption{Row Oriented Value Retrieval}
\begin{algorithmic}[1]
\Function{Read}{$ValueStore, ColIds, Pos, RowEpoch$}
\State $HasInfo, TimestampLength \dashleftarrow ValueStore$
\State $HasCell, HasValue \dashleftarrow ValueStore$
\State $HasTimestamp, HasExpiry, Timestamps, Expiries \gets \emptyset$
\If {$'HasCell' \in HasInfo$}
 \State $HasCell \dashleftarrow ValueStore[\dots $\tiny$\dfrac{ColumnCount}{8})$\scriptsize
\EndIf
\If {$'HasTimestamp' \in HasInfo$}
 \State $HasTimestamp \dashleftarrow ValueStore[\dots $\tiny$\dfrac{\lvert HasCell \rvert}{8})$\scriptsize
\EndIf
\If {$'HasValue' \in HasInfo$}
 \State $HasValue \dashleftarrow ValueStore[\dots $\tiny$\dfrac{\lvert HasCell \rvert}{8})$\scriptsize
\EndIf
\If {$'HasExpiry' \in HasInfo$}
 \State $HasExpiry\dashleftarrow ValueStore[\dots $\tiny$\dfrac{\lvert HasValue \rvert}{8})$\scriptsize
\EndIf
\If {$\lvert HasTimestamp \rvert > 0$}
 \State $Timestamps \dashleftarrow ValueStore[\dots $\tiny$\lvert HasTimestamp \rvert \times TimestampLength)$\scriptsize
\EndIf
\If {$\lvert HasExpiry \rvert > 0$}
 \State $Expiries \dashleftarrow ValueStore[\dots $\tiny$\lvert HasExpiry \rvert \times 8)$\scriptsize
\EndIf
\Statex
\State $Row \gets $ \Call{New}{}

\State $Width, WidthStart, WidthEnd \gets 0, 0, 0$
\For{$ColId \gets ColIds$}
 \If{$ColId \in HasValue$}
  \State $Expiry \gets 0$
  \State $Timestamp \gets RowEpoch$
  \If{$ColId \in HasTimestamp$}
   \State $Timestamp \gets Timestamp + Timestamps[$\Call{Rank}{$ColId, HasTimestamp$}$]$
  \EndIf
  \If{$ColId \notin HasValue$}
   \State $Row \Leftarrow \langle ColId, Timestamp, 'Deleted', Expiry \rangle$
  \Else
   \If{$ColId \in HasExpiry$}
    \State $Expiry \gets Expiries[$\Call{Rank}{$ColId, HasExpiry$}$]$
   \EndIf
   \If{$ColumnWidth(ColId) \neq Width$}
    \State $PrevWidth, Width \gets Width, ColumnWidth(ColId)$
    \For{$Fetch \gets \{ w \mid w \in \mathbf{ran}(Width), PrevWidth < w \leq Width\}$}
     \State $LastColId \gets \max{\{ColId \mid ColumnWidth(ColId) = Fetch\}}$
     \State $WithStart \gets WidthEnd$
     \State $WidthEnd \gets 1 + $ \Call{Rank}{$HasValue,LastColId$}
     \State $Count \gets WidthEnd - WidthStart$
     \State $Values_{Fetch} \dashleftarrow ValueStore[\dots Count \times Fetch)$
    \EndFor
   \EndIf
   \If{$Width \neq \infty$}
    \State $Value \dashleftarrow Values_{Width}[WidthStart + $ \Call{Rank}{$HasValue,ColId$}$]$
   \Else
    \State $Index \gets WidthStart + $ \Call{Rank}{$HasValue,ColId$}
    \State $StartOffset \dashleftarrow Values_{\infty}[Index]$
    \State $EndOffset \dashleftarrow Values_{\infty}[Index + 1]$
    \State $Value \dashleftarrow ValueStore[StartOffset \dots EndOffset)$
   \EndIf
   \State $Row \Leftarrow \langle ColId, Timestamp, Value, Expiry \rangle$
  \EndIf
 \EndIf
\EndFor
\State \Return $Row$
\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage
\paragraph{Column Oriented}
\paragraph{}
Full column-oriented storage supports only fixed-width data types, and is comparatively trivial to define:
each row is assigned an index within the partition, and every single column-oriented store multiplies this
with its base offset and the width of the data type to locate the position where data is stored. No muss, no fuss.
\paragraph{Hybrid}\normalsize{(Fixed-Width Block Encoding)}
\paragraph{}
\small
The idea here is to exploit fixed-width and densely populated data distributions to further compress
data storage requirements, and make indexing to a given row/column involve fewer computational steps.
The basic idea is to store all of the values in a column-oriented fashion within a single data page only,
so a row index would store the page number, and the row offset within the page. The goal is to support
row-oriented workloads, just more efficiently. This model could support non-fixed-width types, but there
is probably little benefit to be had when these could instead be persisted in a row-oriented value store
by placing the fields in a different $Group_N$.
 \begin{itemize}
  \item $TimestampSize$\\[2pt]
    Occupying 2 bits per field in the row, indicates the size of timestamp encoding, with 0 indicating
    all values are equal to the row epoch; 1: 2 bytes; 2: 4 bytes; 3: 8 bytes.
    This must support efficient rank operations, like bitmaps for row-oriented storage, but based on
    cumulative value as opposed to index.
  \item $IsDeleted$ Bitmap \\[2pt]
    Encoded once per row, if $HasDeleted$ is set. A bitmap to be checked before returning the value
    encoded, that indicates if the cell is deleted. Treated as a pseudo column, of fixed width.
  \item $HasDeleted$ Boolean \\[2pt]
    Indicates if $IsDeleted$ is present.
  \item $HasExpiry$ Bitmap\\[2pt]
    Bitmap indicating which columns need to save expiry and deleted information.
  \item $ColumnWidth : ColId \mapsto \mathbb{N}$\\[2pt]
    A table level property indicating which columns are fixed width, and their widths. A value of $\infty$
    indicates the column is dynamic width. Must also support a cumulative value rank based on fixed-width size, 
    but this can be precomputed.
  \item $ColumnCount$\\[2pt]
    A table level property indicating the number of columns persisted against this value store.
\end{itemize}

\begin{algorithm}
\scriptsize
\caption{Hybrid Value Retrieval}
\begin{algorithmic}[1]
\Function{Read}{$ValueStore, ColIds, Pos, RowEpoch$}
\State $TimestampSize, HasExpiry, HasDeleted \dashleftarrow ValueStore$
\State $Row \gets $ \Call{New}{}
\State $IsDeletedId \gets ColumnCount$
\If{$HasDeleted$}
 \State $ColumnWidths \gets ColumnWidths \cup \{ IsDeletedId \mapsto $\tiny$\dfrac{IsDeletedId}{8}$\scriptsize$\}$
 \State $TimestampSize \gets TimestampSize \cup \{ IsDeletedId \mapsto 0\}$
 \State $ColIds \gets ColIds \cup \{ IsDeletedId \}$
\Else
 \State $ColumnWidths \gets ColumnWidths \cup \{ IsDeletedId \mapsto 0\}$
\EndIf

\State $RowLength \gets ColumnWidths(IsDeletedId) + \lvert HasExpiry \rvert \times 4 + \sum{TimestampSize}$
\State $IsDeleted \gets \emptyset$
\If{$HasDeleted$}
 \State $Index \gets RowLength \times (Pos + 1) - ColumnWidth(IsDeletedId)$
 \State $IsDeleted \dashleftarrow ValueStore[Index \dots Index + ColumnWidth(IsDeletedId))$
\EndIf
\For{$ColId \gets ColIds$}
 \State $Index \gets $\Call{Rank}{$HasExpiry, ColId$} $\times 4$
 \State $Index \gets Index + $\Call{Rank}{$HasTimestamp, ColId$}
 \State $Index \gets Index + $\Call{Rank}{$HasDeleted, ColId$}
 \State $Index \gets Index + $\Call{Rank}{$ColumnWidth, ColId$}
 \State $Expiry, Timestamp \gets 0, RowEpoch$
 \If{$ColId \in HasExpiry$}
  \State $Expiry \gets ValueStore[Index\dots Index + 4)$
  \State $Index /gets Index + 4$
 \EndIf 
 \State $Timestamp \gets ValueStore[Index \dots Index+TimestampSize(ColId)]$
 \State $Index \gets Index+TimestampSize(ColId)$
 \If{$ColId \in IsDeleted$}
  \State $Value \gets 'Deleted'$
 \Else
  \State $Value \gets ValueStore[Index \dots Index + ColumnWidth(ColId))$
 \EndIf
 \State $Row \Leftarrow \langle ColId, Timestamp, Value, Expiry \rangle$
\EndFor
\State \Return $Row$
\EndFunction
\end{algorithmic}
\end{algorithm}


\end{document}
